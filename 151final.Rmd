---
title: "Investigating the Predictive Factors of Life Expectancy Across Countries"
author: "Lingjie Chen, Yidong Wu, Yuyin Yang"
date: "2023-12-16"
output:
  pdf_document: default
  html_document: default
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE,message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(corrplot)
library(reshape2)
library(RColorBrewer)
library(GGally)
library(rnaturalearth)
library(sf)
library(countrycode)
library(MASS)
library(outliers)
library(knitr)
```

# Introduction

In recent years, the relationship between socio-economic factors and health has attracted significant attention. Understanding these dynamics is crucial for policymakers and health organizations aiming to improve life expectancy and overall health in various regions. This project focuses on examining how various factors, such as economic conditions, immunization rates, education levels and health-related metrics, influence life expectancy across different countries. It spans a period from 2000 to 2015, covering a diverse range of countries, thereby offering a comprehensive view of global health trends.

The primary objective of this project is to construct a linear model that effectively utilizes a range of explanatory variables to estimate life expectancy. By identifying and quantifying the impact of various socio-economic and health indicators, this project aims to shed light on the crucial factors of life expectancy across nations. This, in turn, can inform targeted interventions and policies to enhance health situations globally.

# Data Description

## Data Source and Collection

The dataset used in this project was initially sourced from Kaggle. It covered a wide range of variables related to life expectancy: health metrics, immunization rates, and socio-economic indicators from 179 countries, covering the years 2000 to 2015. However, the dataset was plagued with severe inaccuracies and missing values.

## Data Update and Validation

Key demographic and economic data such as population, GDP, and life expectancy figures were updated in line with the World Bank's authoritative datasets. Health-related information, including vaccination rates for diseases like Measles, Hepatitis B, Polio, and Diphtheria, along with data on alcohol consumption, Body Mass Index (BMI), HIV incidents, mortality rates, and prevalence of thinness, was carefully compiled from the WHO's public datasets. Additionally, educational metrics, specifically schooling data, were sourced from 'Our World in Data', a project affiliated with the University of Oxford. The updated dataset can be downloaded from [here](https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated).


## Data Structure
We first load the data and change the names of columns for simplicity. Then we take a glimpse of our data.
```{r}
# Load data
raw_data <- read.csv('Life-Expectancy-Data-Updated.csv')
raw_data <- raw_data %>%
  arrange(Country, Year)
colnames(raw_data) <- c("country", "region", "year", "infant_deaths",
                    "under_five_deaths", "adult_mortality", "alcohol",
                    "hepatitis_B", "measles", "bmi", "polio", "diphtheria",
                    "hiv", "gdp", "population", "thinness_ten_nineteen_years",
                    "thinness_five_nine_years", "school", "developed", "developing",
                    "life") 

# Take a glimpse
glimpse(raw_data)
```

The dataset comprises 2,864 rows and 21 columns, representing data from 179 countries for the years 2000 to 2015. It includes 19 numeric variables - 17 quantitative and 2 categorical - providing a well-rounded foundation for analyzing life expectancy determinants. Note that there are two columns named "developed" and "developing", and we will keep one of them when performing the regression.

```{r}
# Create a data frame for variable descriptions
variable_descriptions <- data.frame(
  Variable = c("country", "region", "year", "infant_deaths",
                    "under_five_deaths", "adult_mortality", "alcohol",
                    "hepatitis_B", "measles", "bmi", "polio", "diphtheria",
                    "hiv", "gdp", "population", "thinness_ten_\nnineteen_years",
                    "thinness_five\n_nine_years", "school", "developed", "developing",
                    "life"),
  Description = c("List of the 179 countries",
                  "179 countries are distributed in 9 regions. E.g. Africa, Asia, Oceania, European Union, Rest of Europe and etc.",
                  "Years observed from 2000 to 2015",
                  "Represents infant deaths per 1000 population",
                  "Represents deaths of children under five years old per 1000 population",
                  "Represents deaths of adults per 1000 population",
                  "Represents alcohol consumption that is recorded in liters of pure alcohol per capita with 15+ years old",
                  "Represents % of coverage of Hepatitis B (HepB3) immunization among 1-year-olds",
                  "Represents % of coverage of Measles containing vaccine first dose (MCV1) immunization among 1-year-olds",
                  "BMI is a measure of nutritional status in adults. It is defined as a person's weight in kilograms divided by the square of height in meters",
                  "Represents % of coverage of Polio (Pol3) immunization among 1-year-olds",
                  "Represents % of coverage of Diphtheria tetanus toxoid and pertussis (DTP3) immunization among 1-year-olds",
                  "Incidents of HIV per 1000 population aged 15-49",
                  "GDP per capita in current USD",
                  "Total population in millions",
                  "Prevalence of thinness among adolescents aged 10-19 years. BMI < -2 standard deviations below the median",
                  "Prevalence of thinness among children aged 5-9 years. BMI < -2 standard deviations below the median",
                  "Average years that people aged 25+ spent in formal education",
                  "Developed country",
                  "Developing county",
                  "Average life expectancy of both genders in different years from 2000 to 2015")
)

# Create the table
kable(variable_descriptions, caption = "Description of Variables in the Dataset")
```


## Missing values
```{r}
print(raw_data %>%
  summarise_all(~ sum(is.na(.))))
```
Great! There are no missing values in this dataset.

\newpage

# Exploratory Data Analysis

In this part, we will do several visualizations to help us understand the data better.

## Average Life Expectancy by Region Over Time

```{r,fig.height=4, fig.width=8}
# Calculate the average life expectancy for each region
average_life_by_region <- raw_data %>%
  group_by(region, year) %>%
  summarize(avg_life = mean(life))

# Make the plot
ggplot(average_life_by_region, aes(x = year, y = avg_life, color = region)) +
  geom_line(linewidth=1) +
  labs(x = "Year",
       y = "Average Life Expectancy",) +
  theme(
    legend.position = "top",
  )

# Calculate the average life expectancy for each country
average_data <- raw_data %>%
  group_by(country) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))
average_data <- dplyr::select(average_data, -year, -developing, -country)
```

This line graph illustrates the trend in average life expectancy by region from the year 2000 to 2015. It reveals that all regions had an increase in life expectancy over the given period, with Africa showing a remarkable improvement despite starting from the lowest baseline. Developed regions, such as North America and the European Union, exhibit high life expectancies that appear to plateau, possibly indicating a saturation point where further increases are hard to achieve. The variations between regions suggest a complex interaction of factors affecting life expectancy, including medical care, social policies, and economic conditions.

At this point, we encounter a specific challenge: the data for each country over different years forms a time series with **autocorrelation**, conflicting with the assumption of independence between observations in a linear model. To address this issue, we decide to **average the data across these years for each country**.

## World Map with Life Expectancy

```{r,fig.height=4.5, fig.width=8}
# Obtain global country geographic data
world <- ne_countries(scale = "medium", returnclass = "sf")

# Map country names from the raw data to ISO country codes
raw_data_world <- raw_data %>%
  mutate(country_code = countrycode(country, "country.name", "iso2c"))

# Calculate the average life expectancy for each country
average_life_data <- raw_data_world %>%
  group_by(country_code) %>%
  summarise(average_life = mean(life))

# Make the plot
world_data <- merge(world, average_life_data, by.x = "iso_a2", by.y = "country_code", all.x = TRUE)

ggplot(data = world_data) +
  geom_sf(aes(fill = average_life)) +
  scale_fill_gradientn(colors = brewer.pal(9, "RdBu"), na.value = "lightgrey") +
  theme_minimal() +
  labs(fill = "Life Expectancy") +
  theme(legend.position = "right")
```

The map illustrates stark contrasts in life expectancy across the world. High life expectancy in North America, Europe, and Australia likely reflects robust healthcare, higher living standards, and effective public health policies. In contrast, Africa's lower life expectancy may point to healthcare access challenges, prevalent diseases, and social and economic issues. This visualization highlights the uneven distribution of life expectancy across different countries, revealing potential disparities in wealth, medical resources, nutrition, and governance.

## Correlation Heatmap
```{r, fig.height=10, fig.width=10}
cor_matrix <- cor(average_data, use = "complete.obs")
melted_cor_matrix <- melt(cor_matrix)
color_palette <- colorRampPalette(brewer.pal(11, "RdYlBu"))(100)
ggplot(data = melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.2f", value)), size = 3, color = "black") +
  scale_fill_gradientn(colors = color_palette, name="Pearson\nCorrelation") +
  theme_bw(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, face = "bold", hjust = 1),
        axis.text.y = element_text(face = "bold"),
        plot.title = element_text(hjust = 0.5)) +
  coord_fixed() +
  labs(x = "", y = "", title = "Correlation Heatmap")
```
From the correlation heatmap, we can observe several key factors that potentially influence life expectancy. A strong negative correlation is present with variables such as *adult_mortality*, *infant_deaths*, *under_five_deaths*. These correlations are intuitive as higher mortality rates directly indicate a shorter average lifespan. High infant and under-five death rates often reflect poor health conditions, inadequate healthcare services, and a higher prevalence of infectious diseases, all of which can decrease life expectancy.

In contrast, *school*, *gdp*, *polio* and *diphtheria* exhibit a strong positive correlation with life expectancy. The high vaccination rate can prevent some diseases well, which is directly reflected in the increase of life expectancy. Increased schooling suggests better education and potentially better access to healthcare and health-related information. Higher GDP often reflects a country's wealth, which may afford its citizens better healthcare services, nutrition, and living conditions, contributing to a longer lifespan. 

The heatmap also reveals instances of **multicollinearity**. There is a positive correlation between *school* and *gdp*, suggesting that higher levels of education within a population are often found in countries with higher GDP. In contrast, there is a negative correlation between *school* and *hiv*. This may indicate that higher levels of education lead to greater awareness of HIV and understanding of how to prevent it. Educated individuals might have better access to information on safe practices, which can reduce the rate of HIV transmission. This observation suggests that these factors may lead to larger variance on the estimated coefficients in the model. We will employ some techniques to reduce dimensionality and apply regularization to alleviate the effects of multicollinearity, thereby enhancing the model's accuracy and interpretability. But here, we can observe that the correlation between *infant_deaths* and *under_five_deaths* is so high (0.99), and they represent nearly the same thing. So we decide to **remove** *under_five_deaths*.

```{r}
# Remove the *under_five_deaths* column
average_data_clean <- dplyr::select(average_data, -under_five_deaths)
```


## Pair plot
Since there are too many variables, it's hard to plot all of them. Let's pick some variables that we are interested in.  

```{r,fig.height=10, fig.width=15}
# Make a new dataset to plot
status_data <- average_data_clean
status_data$status <- ifelse(average_data_clean$developed == 1, "Developed",
                              ifelse(average_data$developed == 0 , "Developing", NA))
pairplot_list <- c("life", "bmi", "gdp", "school", "hiv", "adult_mortality", "status")
pairplot_data <- status_data[, pairplot_list]

# Make the plot
ggpairs(
  pairplot_data, 
  aes(colour = status, alpha = 0.8)) + 
  theme_grey(base_size = 18)
```

The pair plot reveals significant correlations among various explanatory variables, supporting the relationships previously discussed, such as the positive correlation between *gdp* and *life_expectancy*, and the negative correlation between *hiv* and *schooling*.

Focusing on the distributions, we can observe that life expectancy is right-skewed for developing countries, indicating that while many individuals have lower life expectancies, there is a tail of the population that lives longer (may be China). For developed countries, the distribution is more symmetric, suggesting a more uniform life expectancy across the population. We can also see the *school* distributions are quite different for developed and developing countries, implying that developed countries have higher levels of education,

The distribution patterns of *gdp* and *hiv* are predominantly right-skewed, suggesting that a majority of countries fall into the lower GDP and HIV rate categories, with notable exceptions at the higher end of the spectrum. Such skewness in the data may require transformations for more accurate regression analysis. It is also crucial to address outliers in the HIV data judiciously, as they have the potential to distort the results of statistical models.

The boxplots at the end compare developed and developing countries directly across these indicators.  Developed countries tend to have higher life expectancy, higher BMI, higher GDP, more schooling, lower HIV prevalence, and lower adult mortality, emphasizing the disparities between developed and developing nations. These differences accentuate the need for policy interventions that are specifically tailored to the distinct socio-economic and health environments of developed and developing countries.

## Distribution of the Rest Variables
```{r,fig.height=25, fig.width=30}
# Make the dataset for the plot
rest_list <- c("infant_deaths", "alcohol",
                    "hepatitis_B", "measles", "polio", "diphtheria",
                    "population", "thinness_ten_nineteen_years",
                    "thinness_five_nine_years", "status") 
rest_data <- status_data[, rest_list]

# Define a plot function
create_den_plot <- function(xvar, xtitle, ytitle){
  ggplot(rest_data, aes_string(x = xvar, fill = "status")) + 
    geom_density(alpha = 0.8) + 
    xlab(ytitle) +  
    ylab('density') +
    ggtitle(paste(ytitle, "by Status")) +
    theme(plot.title = element_text(hjust = 0.5, size = 25, face = "bold"))
}

#Make the plots
dist_columns <- sapply(rest_data, is.numeric)
plot_list <- list()
count <- 0
for(i in which(dist_columns)){
  count <- count + 1
  plot_list[[count]] <- create_den_plot(names(rest_data)[i], "Status", names(rest_data)[i])
}
do.call(grid.arrange, c(plot_list, ncol = 3))
```


We can also see some skewness and outliers. Let's handle them in the next part.

\newpage

# Data Preprocessing

## Transforming Skewness

The following statistic can be used to determine if we need to transform the data:
$$
S = \frac{U-M}{M-L}
$$
This uses the fact that in a symmetric distribution, upper and lower quantiles are equidistant from the median.  
For each numeric column in the dataset, with the exception of "year," "developed," and "developing," we will implement the Box-Cox transformation. 
The Box-Cox transformation is defined as:
$$
X^{(p)}=\begin{cases}
\frac{X^p-1}{p} & p\neq 0\\
\ln(X) &p=0\\
\end{cases}
$$
For columns where $S>1$, we will explore transformation parameters $p\in \{0.5, 0, -0.5, -1, -1.5, -2 \}$. Our goal is to identify the parameter that adjusts $S$ closest to 1. Conversely, for columns with $S<1$, we will evaluate parameters $p\in \{1.5, 2, 2.5, 3, 3.5, 4 \}$ again aiming to achieve $S$ as close to 1 as possible. This approach ensures a more normalized distribution in our dataset, enhancing the robustness and reliability of subsequent analyses.

### Original $S$ before transformation:

```{r}
# Extract the quantitative columns
dist_data <- dplyr::select(average_data_clean, -developed)

# Define a function to compute the skewness statistic
compute_skewness_statistic <- function(x) {
  U <- quantile(x, 0.75)[[1]]
  L <- quantile(x, 0.25)[[1]]
  M <- median(x)
  S <- (U - M) / (M - L)
  return(S)
}

skewness_statistics <- sapply(dist_data, compute_skewness_statistic)
print(skewness_statistics)
```

### Results after transformation:

```{r}
# Define the Box-Cox transformation
box_cox_transformation <- function(x, p) {
  if (p == 0) {
    # Adding a small constant to avoid log(0)
    return(log(x + 1e-8))
  } else {
    return((x^p - 1) / p)
  }
}

# Define a function to select $p$
transform_column <- function(column) {
  S <- compute_skewness_statistic(column)
  p_values <- if (S > 1) c(0.5, 0, -0.5, -1, -1.5, -2) else c(1.5, 2, 2.5, 3, 3.5, 4)
  
  best_p <- p_values[1]
  best_S_diff <- Inf
  transformed_column <- NULL
  
  for (p in p_values) {
    transformed_col_temp <- box_cox_transformation(column, p)
    new_S <- compute_skewness_statistic(transformed_col_temp)
    S_diff <- abs(new_S - 1)
    
    if (S_diff < best_S_diff) {
      best_S_diff <- S_diff
      best_p <- p
      transformed_column <- transformed_col_temp
    }
  }
  
  return(list(transformed_column = transformed_column, best_p = best_p, final_S = compute_skewness_statistic(transformed_column)))
}

# Define a function to transform the dataset and return the results
transform_dataset <- function(dataset) {
  results <- list()
  for (col_name in names(dataset)) {
    column <- dataset[[col_name]]
    transformation_result <- transform_column(column)
    
    dataset[[col_name]] <- transformation_result$transformed_column
    results[[col_name]] <- list(p_value = transformation_result$best_p, final_S = transformation_result$final_S)
  }

  return(list(transformed_data = dataset, transformation_details = results))
}

# Applying the above functions
transform_result <- transform_dataset(dist_data)
transformed_data <- transform_result$transformed_data
transformation_details <- transform_result$transformation_details

#Define a function to print tidier
print_transformation_details <- function(transformation_details) {
  columns <- character()
  p_values <- numeric()
  final_S_values <- numeric()
  
  for (col_name in names(transformation_details)) {
    columns <- c(columns, col_name)
    p_values <- c(p_values, transformation_details[[col_name]]$p_value)
    final_S_values <- c(final_S_values, transformation_details[[col_name]]$final_S)
  }
  
  details_df <- data.frame(Column = columns, P_Value = p_values, Final_S = final_S_values)
  print(details_df)
}

print_transformation_details(transformation_details)
```


We noted that for right-skewed columns ($S<1$), $p=0$ is generally effective, with the exception of the *hiv* column which requires $p=-0.5$. For simplicity, we will apply $p=0$ to all right-skewed columns. 

However, we encountered challenges with several left-skewed columns such as *bmi*, *polio*, *diphtheria*, and *life*. Despite applying larger transformation parameters, $S$ of these columns did not approach 1 as intended. This deviation might be attributed to the presence of outliers in these data sets. Our next step take a close look to these outliers to ensure more accurate and representative data analysis.

```{r}
# Following the criterions, redo transformation for *hiv*
transformed_data$hiv <- log(average_data$hiv)
```


## Univariate Outliers

We first check the box-plot of these columns in the dataset before the transformation: *bmi*, *polio*, *diphtheria*, and *life*.

```{r,fig.height=7.5, fig.width=10}
create_boxplot <- function(yvar, ytitle){ 
  data_stats <- function(y) {
    Q1 <- quantile(y, 0.25, na.rm = TRUE)
    Q3 <- quantile(y, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    list(lower_inner_fence = Q1 - 1.5 * IQR,
         upper_inner_fence = Q3 + 1.5 * IQR,
         lower_outer_fence = Q1 - 3 * IQR,
         upper_outer_fence = Q3 + 3 * IQR)
  }
  stats <- data_stats(average_data[[yvar]])
  
  ggplot(average_data, aes_string(y = yvar)) +
    geom_boxplot() + 
    geom_hline(yintercept = stats$lower_inner_fence, color = "blue", linetype = "dashed") +
    geom_hline(yintercept = stats$upper_inner_fence, color = "blue", linetype = "dashed") +
    geom_hline(yintercept = stats$lower_outer_fence, color = "red", linetype = "dashed") +
    geom_hline(yintercept = stats$upper_outer_fence, color = "red", linetype = "dashed") +
    ylab(ytitle) +
    theme(plot.title = element_text(hjust = 0.5, size = 20, face = "bold"))
}

box_plot_col <- c("bmi", "polio", "diphtheria", "life")
plot_list <- list()

for (col in box_plot_col) {
  plot_title <- paste("Box Plot of", col)
  plot_list[[length(plot_list) + 1]] <- create_boxplot(col, plot_title)
}

do.call(grid.arrange, c(plot_list, ncol = 2))
```

The univariate outliers observed across the datasets indicate significant deviations. Their impact is considerable to potentially distort the overall analysis. However, we must be cautious in removing any outliers, as they may not be regression outliers and could hold special significance. For instance, outliers in 'bmi' might represent some developed countries, while outliers in other variables could represent some poorer nations. Therefore, we merely acknowledge that these points **might** affect the regression results, but we do not remove them. We will review them again during regression diagnostics.

## Standardizing the data

Standardizing ensures that all features contribute equally to the model, preventing variables with larger scales from disproportionately influencing the results. It also improves the **interpretability** of coefficients, as they can be compared on the same scale. Additionally, standardizing is crucial for PCA to work well later.

```{r}
data_clean_scaled <- as.data.frame(scale(transformed_data))
data_clean_scaled$developed <- average_data$developed
```

Now, we can perform the MLR. Let's take a look.

```{r}
summary(lm(life ~., data=data_clean_scaled))
```



\newpage

# Model Selection

## Main Effect Variable selection

First, we carry out the main effect variable selection using `regsubset()` function.

```{r}
library(leaps)
vs <- regsubsets(life ~., data = data_clean_scaled, nbest = 1, method = "exhaustive")
rs <- summary(vs)
rs$which
```

Then, we calculate the AIC and BIC for all these 8 models for reference and choose the best one among them.

```{r,width=40}
library(boot)
 subset_aic_values <- sapply(1:nrow(rs$which), function(i) {
  model <- lm(life ~ ., data = data_clean_scaled[,c('life',names(which(rs$which[i,]==TRUE)[-1]))])
  AIC(model)
})
subset_bic_values <- sapply(1:nrow(rs$which), function(i) {
  model <- lm(life ~ ., data = data_clean_scaled[,c('life',names(which(rs$which[i,]==TRUE)[-1]))])
  BIC(model)
})
best_aic <- which.min(subset_aic_values)
best_bic <- which.min(subset_bic_values)
cat("Best model by AIC includes:\n", paste(names(which(rs$which[which.min(subset_aic_values),]==TRUE)[-1]), collapse = ", "), "\n")
cat("Best model by BIC includes:\n", paste(names(which(rs$which[which.min(subset_bic_values),]==TRUE)[-1]), collapse = ", "), "\n")
print(paste("The best model from AIC metric is:",which.min(subset_aic_values)))
print(paste("The best model from BIC metric is:",which.min(subset_bic_values)))
print(paste("The best model's AIC value is:",min(subset_aic_values)))
print(paste("The best model's BIC value is:",min(subset_bic_values)))
```

However, the `regsubset()` function inherently possesses computational complexity constraints. As a result, this method limits the variable combinations to a maximum of eight, which could potentially be insufficient for our comprehensive variable selection needs. The limitation in the scope of variable combination could lead to overlooking some vital variables that might be crucial for the model.

Considering a critical factor in our decision was the performance of the models in terms of their AIC and BIC values. So that we carry out an exhaustive research using AIC and BIC as evaluation and select the best variable set. Conceptually, it will serve as a more robust and effective approach for our variable selection process.

```{r}
full_model <- lm(life ~ ., data=data_clean_scaled)
predictors <- names(data_clean_scaled)[names(data_clean_scaled) != "life"]

# Create all combinations of variables with minimal at 8
combinations <- unlist(lapply(8:length(predictors), function(i) combn(predictors, i, simplify = FALSE)), recursive = FALSE)

aic_values <- vector("list", length(combinations))
bic_values <- vector("list", length(combinations))

# Carry out exhaustive search on all combinations
for(i in seq_along(combinations)) {
    formula <- as.formula(paste("life ~", paste(combinations[[i]], collapse = "+")))
    model <- lm(formula, data=data_clean_scaled)
    aic_values[[i]] <- AIC(model)
    bic_values[[i]] <- BIC(model)
}

best_model_aic <- combinations[[which.min(unlist(aic_values))]]
best_model_bic <- combinations[[which.min(unlist(bic_values))]]

cat("Best model by AIC includes:\n", paste(best_model_aic, collapse = ", "), "\n")
cat("The best model's AIC value is:", min(unlist(aic_values)), "\n")
cat("Best model by BIC includes:\n", paste(best_model_bic, collapse = ", "), "\n")
cat("The best model's BIC value is:", min(unlist(bic_values)), "\n")
```
In conclusion, the model selected by AIC has 9 explanatory variables, namely infant_deaths, adult_mortality, alcohol, measles, diphtheria, gdp, population, thinness_ten_nineteen_years and the dummy variable developed. The model selected by BIC drop `gdp` and `thinness_ten_nineteen_years` due to larger penalty on variable numbers. 
To decide which model to choose as our final model, we do F test on `gdp` and ``thinness_ten_nineteen_years`, checking whether they are significant.

```{r}
full_model = lm(life ~ infant_deaths + adult_mortality + alcohol + measles + diphtheria + gdp + population + thinness_ten_nineteen_years + developed, data=data_clean_scaled)
reduced_model1 = lm(life ~ infant_deaths + adult_mortality + alcohol + measles + diphtheria + gdp + population + developed, data=data_clean_scaled)
anova(reduced_model1, full_model)
reduced_model2 = lm(life ~ infant_deaths + adult_mortality + alcohol + measles + diphtheria + population + thinness_ten_nineteen_years + developed, data=data_clean_scaled)
anova(reduced_model2, full_model)
reduced_model3 = lm(life ~ infant_deaths + adult_mortality + alcohol + measles + diphtheria + population + developed, data=data_clean_scaled)
anova(reduced_model3, full_model)
```
The p-values for H0: $\beta_{gdp}=0$ and $\beta_{thin}=0$ are 0.06 and 0.16 repectively. The The p-value for H0: $\beta_{gdp}=\beta_{thin}=0$ is 0.07, indicating that we can't reject the null hypothesis. So we exclude both `gdp` and `thinness_ten_nineteen_years` in our model.
Above all, we choose the variables selected by the BIC searching method with 7 explanatory variables: infant_deaths, adult_mortality, alcohol, measles, diphtheria, population, developed. 

## Interaction Terms Selection

```{r,echo=FALSE}

library(car)

selected_vars <- c("adult_mortality","infant_deaths", "alcohol", "measles", "diphtheria", "population", "developed")
quantitative_vars <- c("adult_mortality","infant_deaths", "alcohol", "measles", "diphtheria", "population")
categorical_var <- "developed"


main_effects_formula <- reformulate(termlabels = paste(selected_vars, collapse = " + "), response = "life")
# print(main_effects_formula)

# Differentiate all the interaction terms
interaction_terms <- paste(quantitative_vars, categorical_var, sep = ":", collapse = " + ")
main_effects_str <- paste(deparse(main_effects_formula), collapse = " ")
full_model_formula <- paste(main_effects_str, "+", paste(interaction_terms, collapse = " + "))
# print(full_model_formula)

full_model <- lm(full_model_formula, data = data_clean_scaled)

f_test_results <- list()
models_without_interaction <- list()

interaction_terms <- setdiff(names(coef(full_model)), c("(Intercept)", selected_vars))

for (term in interaction_terms) {
    reduced_formula_str <- paste(main_effects_str, "+", paste(interaction_terms, collapse = " + "), "- ", term)
    reduced_model_formula <- as.formula(reduced_formula_str)
    reduced_model <- lm(reduced_model_formula, data = data_clean_scaled)
    
    f_test_results[[term]] <- anova(reduced_model, full_model)
    models_without_interaction[[term]] <- reduced_model
}
```

```{r,include=FALSE}
# The results will show in the console
print(f_test_results)
```

In this section, we evaluate the significance of various interaction terms derived from the main effect variables identified in earlier analyses. Our methodology does not incorporate forward or backward selection techniques; instead, we aim to directly assess the explanatory power of each interaction term with respect to the response variable. We select the interaction terms with its p-values in the ANOVA, using the threshold of 0.05. We select *adult_mortality:developed* and *alcohol:developed* as our additional interaction variables. This approach is selected over **backward selection** to avoid the potential instability and model volatility often associated with **one-step** selection methods. By focusing on the interaction terms individually, we intend to determine their individual contributions to the model's predictive capacity, ensuring a robust and theoretically sound model construction and not nelegcting any important explanatary variable.(Check the Rmd file for exact F-test output)

## Interpretation for the Interaction Terms

- **Adult Mortality and Development Status:**

The interaction between adult mortality and development status is significant, with a p-value of 0.04618. This suggests that the effect of adult mortality on life expectancy is significantly different between developed and developing countries, with a strong indication that reducing adult mortality could have a different effect on increasing life expectancy in one group compared to the other. Which is intuitive since the baseline of *adult_mortality* between developing and developed countries aren't the same.

- **Alcohol and Development Status:**

The interaction is also significant for alcohol and development status, with a p-value of 0.01878. This highlights the varying impact of alcohol on life expectancy between developed and developing countries, implying that having access to alcohol has different meaning in different countries. Its effect on *life* may be shown by different ways in developed and developing countries

- **Non-Significant Interaction Terms:**

The interactions involving *infant_deaths*, *measles*, *diphtheria* and *population* with development status, though not statistically insignificant, have higher p-values compared to the aforementioned factors. This implies that these factors impact life expectancy more uniformly across different development statuses and might not need as differentiated an approach as adult mortality or under-five deaths. 

- **Implications for Public Health Policy:**

The analysis implies that public health interventions should not adopt a one-size-fits-all strategy. Instead, they should be tailored to address the specific challenges faced by developed and developing countries. For instance:

1. In developing countries, a focused strategy on reducing adult mortality and decreasing the malnutrition youth's proportion could lead to significant improvements in life expectancy.

2. The significant interaction with alcohol suggests that developing countries may need to prioritize alcohol access in life. expectancy.

3. Factors like population infectious diseases may require global health policies that are effective across different developmental contexts, as their impact seems to be more uniform across countries.

\newpage

# Regression Diagnostics

In this part, we first regress on our selected main variables to test its performance and set as a benchmark for following refined models.

```{r}
best_main_model_variables = c("adult_mortality","infant_deaths", "alcohol", "measles", "diphtheria", "population", "developed")
best_main_formula <- as.formula(paste("life ~", paste(best_main_model_variables, collapse = "+")))
best_main_model <- lm(best_main_formula, data=data_clean_scaled)
summary(best_main_model)
```
  

Examination of the coefficients' p-values reveals that all but one variable demonstrate statistical significance within this model. Notably, compared to our initial model which included the entire set of variables, the current model exhibits substantially enhanced interpretability. This improvement significantly augments our capacity to conduct a detailed analysis of the dataset, thereby allowing for the extraction of more valuable insights. 

Next, we incorporate the interaction terms into the model through the **one-step-backward selection** process and check its performance.

```{r}
best_model_variables = c("adult_mortality","infant_deaths", "alcohol", "measles", "diphtheria", "population", "developed","alcohol:developed","adult_mortality:developed")
best_formula <- as.formula(paste("life ~", paste(best_model_variables, collapse = "+")))
best_model <- lm(best_formula, data=data_clean_scaled)
summary(best_model)
```


The coefficients obtained align closely with our initial assumptions and the results of our selection criteria. Notably, the p-value associated with the variable *developed* is relatively high, suggesting it does not hold statistical significance in this particular model.

However, its inclusion remains justified based on the **Principle of Marginality**, which stipulates that main effects should be retained in a model when interaction terms are present. Furthermore, the apparent insignificance of *developed* after the inclusion of interaction terms may be attributed to the interaction terms themselves encapsulating the main effect of *developed* on life expectancy. This observation potentially highlights the differential impact of various variables on life expectancy between developing and developed countries, underscoring the nuanced nature of these relationships within the model's framework.

Furthermore, the model summary highlights several key findings:

- The coefficients for *adult_mortality*, *infant_deaths*, *alcohol*, *measles*, *diphtheria*, and *population* are statistically significant, with *adult_mortality* showing the most substantial negative impact on life expectancy.

- Interaction terms *alcohol:developed* and *adult_mortality:developed* are significant, suggesting that the effect of *alcohol* and *adult_mortality* on life expectancy differs between developed and developing countries.

- The overall model has a high Multiple R-squared value, indicating that it accounts for a large proportion of the variability in life expectancy. And the model's F-statistic is highly significant, which provides strong evidence that the model is a good fit for the data.

Then we generate some diagnostic plots:

```{r,fig.height=6,fig.width=6}
# Generate diagnostic plots
par(mfrow=c(2, 2))
plot(best_model, which = 1) # Residuals vs Fitted
plot(best_model, which = 3) # Scale-Location
plot(best_model, which = 2) # Normal Q-Q
plot(best_model, which = 5) # Residuals vs Leverage
```

From the plots we can conclude that:

The uniformly distributed residuals can ensure the validity of the linear model assumptions. Since there is no apparent pattern, which suggests that the variance of the residuals is constant, and the linearity assumption is reasonable. And the Scale-Location plot shows that residuals spread equally along the ranges of predictors. This is indicative of homoscedasticity which aligns with the residual plot's result.

Furthermore, the Normal Q-Q plot displays how well the residuals match a normal distribution. The points largely follow the reference line, suggesting that the residuals are normally distributed, but there are still some deviations in the tails, which could be outliers and need further treatment.

## Outlier elimination

In this part, we focus on eliminating the **regression outliers** from the dataset. The rubric we are using here is the standard residuals and Cook's distance. Then we refit the model and start further diagnostics.

```{r,fig.height=6,fig.width=6}
# Remove the outliers and refit
std_residuals <- rstandard(best_model)

cooks_d <- cooks.distance(best_model)

# We tried out different combination of threshold here, the following one performs the best among all

threshold_std_resid <- 2 
threshold_cooks_d <- 4 / nrow(data_clean_scaled) 

outliers <- which(abs(std_residuals) > threshold_std_resid | cooks_d > threshold_cooks_d)

data_clean <- data_clean_scaled[-outliers, ]

best_model_refit <- lm(formula(best_model), data=data_clean)

# Generate diagnostic plots
par(mfrow=c(2, 2))

plot(best_model_refit, which = 1) # Residuals vs Fitted
plot(best_model_refit, which = 3) # Scale-Location
plot(best_model_refit, which = 2) # Normal Q-Q
plot(best_model_refit, which = 5) # Residuals vs Leverage

summary(best_model_refit)
```


### Improvement

The diagnostic plots for the refined model suggest substantive improvements following the elimination of outliers. Specifically:

- The **Residuals vs Fitted** plot exhibits a more randomized dispersion of residuals about the horizontal axis, indicative of enhanced homoscedasticity and adherence to linearity assumptions.
- The **Scale-Location** plot displays a uniform spread of standardized residuals across the range of fitted values, signaling a reduction in heteroscedasticity.
- The **Normal Q-Q** plot demonstrates that residuals more closely follow the theoretical quantiles, suggesting better normality.
- The **Residuals vs Leverage** plot shows that points with high leverage have diminished influence on the model, with Cook's distances remaining within acceptable limits.

Collectively, these diagnostic improvements suggest that the model's assumptions are better satisfied post-outlier removal.

The model's high Multiple R-squared and significant F-statistic post-refinement underscore its improved fit and interpretability, which is much larger than the R-square of the model including all variables, meaning our model has extract the structure and main component from the data. Despite the non-significance of *developed*, the model's stability and predictive power are enhanced, making it a more robust tool for predicting life expectancy.

# Final Model

$$
\begin{aligned}
\text{LifeExpectance}^{4} = & \  -0.54918 \\
& - 2.61264 \times \log(\text{adult\_mortality}) \\
& - 1.59636 \times \log(\text{infant\_deaths}) \\
& + 0.7496 \times \text{alcohol}^{0.5} \\
& - 0.05343      \times\text{measles}^{4} \\
& + 0.05541     \times \text{diphtheria}^{4}  \\
& + 0.1762       \times \log(\text{population}) \\
& + 1.49508    \times \text{developed} \\
& + 0.3668     \times \log(\text{adult\_mortality}) \times \text{developed} \\
& - 1.23416 \times \text{alcohol}^{0.5} \times \text{developed} \\
\end{aligned}
$$
The variables included in the final regression model have been standardized, which precludes the model's immediate application for predictive purposes in its current form. It should be noted that the primary objective of this project is to examine the interrelations among the variables through regression analysis rather than to emphasize predictive accuracy. 

## Analysis of Regression Coefficients

- **Adult Mortality:**

The coefficient for the log-transformed adult mortality is negative (-0.63080), indicating that as adult mortality rates increase, life expectancy decreases. The logarithmic transformation suggests this relationship is nonlinear, with diminishing impact as mortality rates rise.

- **Alcohol Consumption:**

Alcohol consumption has a positive coefficient (0.13244) when transformed with a square root, which may reflect a complex relationship where moderate levels of alcohol consumption correlate with higher life expectancy, perhaps due to social and lifestyle factors.

- **Development Status:**

The binary variable for development status shows a significant positive coefficient (0.34341), implying that life expectancy is higher in developed countries.

- **Interaction Terms:**

1. The interaction between the log-transformed adult mortality and development status is positive (0.10206), suggesting that the negative impact of adult mortality on life expectancy may be lessened in developed countries.
2. The interaction term for square-root-transformed alcohol consumption and development status is negative (-0.27348), which could indicate that the positive relationship between alcohol and life expectancy in the general model is not as strong or is reversed in developed countries.

# Additional Work
## Using `Region` As a Dummy Variable in Regression

In the previous discussion, we calculated the average of various quantitative variables for each country over years as the explanatory variables and employed average life expectancy over years as the response variable. We solely incorporated `developed` as a dummy variable while overlooking the variable `Region`. The reason we excluded `Region` in MLR is that the number of regions is too large and including this dummy variable may result in a large number of variables, leading to unstable estimates of coefficients. Our underlying assumption is that the variations of life expenctency between regions could be accounted for by disparities among countries in other variables.

In this section, we introduce `Region` as an additional dummy variable revisit the earlier discussion. We will compare this model with the best model obtained in the preceding analysis.

### Model Selection

```{r,include=FALSE}
# Data Processing
average_data_region <- raw_data %>%
  group_by(country) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), region = first(region))
average_data_region$isCentralAmerica = ifelse(average_data_region$region == "Central America and Caribbean",1,0)
average_data_region$isSouthAmerica = ifelse(average_data_region$region == "South America",1,0)
average_data_region$isAfrica = ifelse(average_data_region$region == "Africa",1,0)
average_data_region$isMiddleEast = ifelse(average_data_region$region == "Middle East",1,0)
average_data_region$isOceania = ifelse(average_data_region$region == "Oceania",1,0)
average_data_region$isAsia = ifelse(average_data_region$region == "Asia",1,0)
average_data_region$isEuropeanUnion = ifelse(average_data_region$region == "European Union",1,0)
average_data_region$isNorthAmerica = ifelse(average_data_region$region == "North America",1,0)
#data_clean_scaled_region$isRestEurope = ifelse(data_clean_scaled_region$region == "Rest of Europe",1,0)

average_data_region <- dplyr::select(average_data_region, -year, -developing, -country, -under_five_deaths, -region)
dist_data_region <- dplyr::select(average_data_region, -developed, -isCentralAmerica, -isSouthAmerica, -isAfrica, -isMiddleEast, -isOceania, -isAsia, -isEuropeanUnion, -isNorthAmerica)
transform_result_region <- transform_dataset(dist_data_region)
transformed_data_region <- transform_result_region$transformed_data
transformed_data_region$hiv <- log(average_data_region$hiv)
data_clean_scaled_region <- as.data.frame(scale(transformed_data_region))
data_clean_scaled_region$developed <- average_data_region$developed
data_clean_scaled_region$isSouthAmerica <- average_data_region$isSouthAmerica
data_clean_scaled_region$isAfrica <- average_data_region$isAfrica
data_clean_scaled_region$isMiddleEast <- average_data_region$isMiddleEast
data_clean_scaled_region$isOceania <- average_data_region$isOceania
data_clean_scaled_region$isAsia <- average_data_region$isAsia
data_clean_scaled_region$isEuropeanUnion <- average_data_region$isEuropeanUnion
data_clean_scaled_region$isNorthAmerica <- average_data_region$isNorthAmerica
data_clean_scaled_region$isCentralAmerica <- average_data_region$isCentralAmerica

# Model Selection (like before)
vs <- regsubsets(life ~., data = data_clean_scaled_region, nbest = 1, method = "exhaustive")
rs <- summary(vs)
rs$which
library(boot)
 subset_aic_values <- sapply(1:nrow(rs$which), function(i) {
  model <- lm(life ~ ., data = data_clean_scaled_region[,c('life',names(which(rs$which[i,]==TRUE)[-1]))])
  AIC(model)
})
subset_bic_values <- sapply(1:nrow(rs$which), function(i) {
  model <- lm(life ~ ., data = data_clean_scaled_region[,c('life',names(which(rs$which[i,]==TRUE)[-1]))])
  BIC(model)
})
best_aic <- which.min(subset_aic_values)
best_bic <- which.min(subset_bic_values)
cat("Best model by AIC includes:\n", paste(names(which(rs$which[which.min(subset_aic_values),]==TRUE)[-1]), collapse = ", "), "\n")
cat("Best model by BIC includes:\n", paste(names(which(rs$which[which.min(subset_bic_values),]==TRUE)[-1]), collapse = ", "), "\n")
print(paste("The best model from AIC metric is:",which.min(subset_aic_values)))
print(paste("The best model from BIC metric is:",which.min(subset_bic_values)))
print(paste("The best model's AIC value is:",min(subset_aic_values)))
print(paste("The best model's BIC value is:",min(subset_bic_values)))

```

We use the same methods in previous work for model selection found the model with the smallest BIC is `lm(life ~ infant_deaths + adult_mortality + population + school + regionCentral America and Caribbean + regionSouth America)` and the smallest AIC model is `lm(life ~ infant_deaths + adult_mortality + population + measles +  diphtheria + school + isCentralAmerica + isSouthAmerica) `. We use similar F-test method to determine whether we drop `measles` and `diphtheria` in our model.

```{r,include=FALSE}
full_model = lm(life ~ infant_deaths + adult_mortality + measles + diphtheria + population + school + isCentralAmerica+ isSouthAmerica, data=data_clean_scaled_region)
reduced_model1 = lm(life ~ infant_deaths + adult_mortality + diphtheria + population + school + isCentralAmerica+ isSouthAmerica, data=data_clean_scaled_region)
anova(reduced_model1, full_model)
reduced_model2 = lm(life ~ infant_deaths + adult_mortality + measles + population + school + isCentralAmerica + isSouthAmerica, data=data_clean_scaled_region)
anova(reduced_model2, full_model)
reduced_model3 = lm(life ~ infant_deaths + adult_mortality + population + school + isCentralAmerica + isSouthAmerica, data=data_clean_scaled_region)
anova(reduced_model3, full_model)
```

The results show that we can reject all the three null hypothesis with the p value of 0.02028, 0.01243 and 0.0173 respectively. So our selected model is `lm(life ~ infant_deaths + adult_mortality + population + measles +  diphtheria_school + isCentralAmerica + isSouthAmerica)`.

### Interaction selection

We evaluate the significance of various interaction terms and choose the only significant term: `adult_mortality:isCentralAmerica`, with a p value of 0.002349.

```{r,include=FALSE}

# Interaction term selection
selected_vars <- c("adult_mortality","infant_deaths", "school", "population", "measles", "diphtheria","isCentralAmerica", "isSouthAmerica")
quantitative_vars <- c("adult_mortality","infant_deaths", "population","school", "measles", "diphtheria")
categorical_var <- c("isCentralAmerica", "isSouthAmerica")


main_effects_formula <- reformulate(termlabels = paste(selected_vars, collapse = " + "), response = "life")
# print(main_effects_formula)

# Differentiate all the interaction terms
full_model_formula <- paste(main_effects_str, "+", "adult_mortality:isCentralAmerica + infant_deaths:isCentralAmerica + population:isCentralAmerica + school:isCentralAmerica + adult_mortality:isSouthAmerica + infant_deaths:isSouthAmerica + population:isSouthAmerica + school:isSouthAmerica + measles:isCentralAmerica + diphtheria:isCentralAmerica + measles:isSouthAmerica + diphtheria:isSouthAmerica")
# print(full_model_formula)

full_model <- lm(full_model_formula, data = data_clean_scaled_region)

f_test_results <- list()
models_without_interaction <- list()

interaction_terms <- setdiff(names(coef(full_model)), c("(Intercept)", selected_vars))

for (term in interaction_terms) {
    reduced_formula_str <- paste(main_effects_str, "+", paste(interaction_terms, collapse = " + "), "- ", term)
    reduced_model_formula <- as.formula(reduced_formula_str)
    reduced_model <- lm(reduced_model_formula, data = data_clean_scaled_region)
    
    f_test_results[[term]] <- anova(reduced_model, full_model)
    models_without_interaction[[term]] <- reduced_model
}
print(f_test_results)
# We found that only `adult_mortality:isCentralAmerica` is significant, with a p value of 0.002601.
best_formula_region <- "life ~ infant_deaths + adult_mortality + population + measles +  diphtheria + school + isCentralAmerica + isSouthAmerica + adult_mortality:isCentralAmerica"
best_model_region <- lm(best_formula_region, data=data_clean_scaled_region)
summary(best_model_region)
```

### Regression Diagnostics

With this final model, we get the model of `life ~ infant_deaths + adult_mortality + population + measles +  diphtheria + school + isCentralAmerica + isSouthAmerica + adult_mortality:isCentralAmerica`. We do model diagnose as before and remove outliers.
```{r,}
std_residuals <- rstandard(best_model_region)

cooks_d <- cooks.distance(best_model_region)

# We tried out different combination of threshold here, the following one performs the best among all

threshold_std_resid <- 2 
threshold_cooks_d <- 4 / nrow(data_clean_scaled) 

outliers <- which(abs(std_residuals) > threshold_std_resid | cooks_d > threshold_cooks_d)

data_clean_region <- data_clean_scaled_region[-outliers, ]
```

The result of the model on the cleaned dataset is as below.
```{r}
best_model_refit_region <- lm(formula(best_model_region), data=data_clean_region)
summary(best_model_refit_region)
```
The interaction term become not significant when we fit the model on the cleaned dataset, indicating that the previous low p value is probably caused by outliers. So we exclude the interaction term in our regression and get the final formula: `life ~ infant_deaths + adult_mortality + population + measles +  diphtheria + school + isCentralAmerica + isSouthAmerica`.

The results for this model are as below:
```{r}
best_model_refit_region <- lm(life ~ infant_deaths + adult_mortality + population + measles +  diphtheria + school + isCentralAmerica + isSouthAmerica, data=data_clean_region)
summary(best_model_refit_region)
```


```{r,fig.height=6,fig.width=6}
# Generate diagnostic plots
par(mfrow=c(2, 2))

plot(best_model_refit_region, which = 1) # Residuals vs Fitted
plot(best_model_refit_region, which = 3) # Scale-Location
plot(best_model_refit_region, which = 2) # Normal Q-Q
plot(best_model_refit_region, which = 5) # Residuals vs Leverage
```

### Model Comparison and Analysis
#### Final Model
$$
\begin{aligned}
\text{LifeExpectance}^{4} = & \  -0.4055 \\
& - 2.456768 \times \log(\text{adult\_mortality}) \\
& - 1.397716 \times \log(\text{infant\_deaths}) \\
& - 0.042356         \times\text{measles}^{4} \\
& + 0.042304        \times \text{diphtheria}^{4}  \\
& + 0.187492       \times \log(\text{population}) \\
& + 0.204272    \times \text{school}^{1.5} \\
& + 1.397608    \times \text{isCentralAmerica} \\
& + 0.900924    \times \text{isSouthAmerica} \\
\end{aligned}
$$


#### Selected Model Variables Comparison
We compare the model with region information with our previous model and find that apart from the region dummy variable, the two model lay emphasis on different explanatory variables. The new model excluded *alcohol* and the dummy variable *developed*, including *school* and regional dummy variables instead.
The exclusion of the `developed` variable may be explain by the introduction of regional dummy variables. It indicates that regional factor can better explain the variance between different countries than the developed status. 
Also, the countries within the same region tend to share the same developed or developing status, alcohol consumption, so there may be no need to include those variables again when we use regional dummy variables.

#### Coefficient Comparison and Interpretation
The estimate for the coefficient of the variables are quite similar comparing the two model. 
The new model chooses two regional dummy variables: *isCentralAmerica* and *isSouthAmerica*, indicating whether the country is in central America and Caribbean or in South America. It's surprising that the model didn't include indicators for the country located in Africa (where the life expectancy is lowest) or North America (where the life expectancy is highest). The model shows that holding all the other variables constant, the country in Central America have 0.349 higher average life expectancy and the South American countries witness a 0.225 rise. These rise may be caused by variables outside the data.

#### Performance Comparison
Comparing the two model, the model with regional information has a slightly higher adjusted R square value (0.9876 vs 0.9829) with less explanatory variables. Also in the new model, we can see that all the variables are highly significant, proving that the model is strongly convincing. 
However, the model didn't include the information of development status and its interaction with alcohol assumption and adult mortality. Since the objective of this project is to examine the interrelations among the variables through regression analysis and to get insight for the influence of variables on life expectancy, we may lose those precious information. However, if the reader would like to choose a model for more accurate prediction performance, we suggest using the model with regional information since it has a higher adjusted R squared value.

## Dimensionality reduction with PCR
Another way to avoid multicollinearity and mitigate overfitting is to instead use principal components regression, which finds M linear combinations (known as principal components) of the original p predictors and then uses least squares to fit a linear regression model using the principal components as predictors.
We set `scale=FALSE` since we've already scaled the data. We use k-fold cross-validation to evaluate the performance of the model. (k = 10 by default)

```{r}
library(pls)
set.seed(1)
#fit PCR model
data_clean_scaled2 <- na.omit(data_clean_scaled)
pcr_model <- pcr(life ~ .-life-developed, data = data_clean_scaled2,scale=TRUE, validation="CV")
pcr_summary=summary(pcr_model)
```
### Principal Component Number Selection
Once weve fit the model, we need to determine the number of principal components worth keeping.
The way to do so is by looking at the test root mean squared error (test RMSE) and calculated by the k-fold cross-validation and the percentange of variance explained

1. RMSE
This table tells us the test RMSE calculated by the k-fold cross validation. We can see the following:
If we only use the intercept term in the model, the test RMSE is 0.3646.
If we add in the first principal component, the test RMSE drops to 0.2363.
If we add in the second principal component, the test RMSE drops to 0.2270.
We can see that adding additional principal components actually leads to an decrease in test RMSE. However, after the number of components reaches 6, the improvement is slight.

```{r,fig.height=4,fig.width=8}
validationplot(pcr_model, val.type="RMSEP")
```

2. Percentange of Variance Explained
This table tells us the percentage of the variance in the response variable explained by the principal components. Well be able to explain more variance by using more principal components. By adding in the top 6 principal component, we can explain over 90% (91.23%) of the variation in the response variable, and we can see that adding in more than 6 principal components doesnt actually increase the percentage of explained variance by much.


```{r,fig.height=4,fig.width=8}
validationplot(pcr_model, val.type="R2")
```

### Model comparison and Analysis

We exclude all dummy variables in PCA since it's harder to interprete the model once these dummy variables are included. However, we can add the principal components we choose with selected dummy variables in our final model to boost the model predictive performance.

```{r}
data.pca <- prcomp(data_clean_scaled_region[, !colnames(data_clean_scaled_region) %in% c("life","developed","isCentralAmerica", "isSouthAmerica", "isAfrica", "isMiddleEast", "isOceania", "isAsia", "isEuropeanUnion", "isNorthAmerica")], center=TRUE, scale.=FALSE)
data_pca_6 <- cbind(data_clean_scaled_region[,colnames(data_clean_scaled_region) %in% c("life","developed","isCentralAmerica", "isSouthAmerica", "isAfrica", "isMiddleEast", "isOceania", "isAsia", "isEuropeanUnion", "isNorthAmerica")], data.pca$x[,1:6])
selected_vars <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","developed","isCentralAmerica", "isSouthAmerica", "isAfrica", "isMiddleEast", "isOceania", "isAsia", "isEuropeanUnion", "isNorthAmerica")
quantitative_vars <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8")
categorical_var <- c("developed","isCentralAmerica", "isSouthAmerica", "isAfrica", "isMiddleEast", "isOceania", "isAsia", "isEuropeanUnion", "isNorthAmerica")

vs <- regsubsets(life ~., data = data_pca_6, nbest = 1, method = "exhaustive")
rs <- summary(vs)
 subset_aic_values <- sapply(1:nrow(rs$which), function(i) {
  model <- lm(life ~ ., data = data_pca_6[,c('life',names(which(rs$which[i,]==TRUE)[-1]))])
  AIC(model)
})
subset_bic_values <- sapply(1:nrow(rs$which), function(i) {
  model <- lm(life ~ ., data = data_pca_6[,c('life',names(which(rs$which[i,]==TRUE)[-1]))])
  BIC(model)
})
best_aic <- which.min(subset_aic_values)
best_bic <- which.min(subset_bic_values)
cat("Best model by AIC includes:\n", paste(names(which(rs$which[which.min(subset_aic_values),]==TRUE)[-1]), collapse = ", "), "\n")
cat("Best model by BIC includes:\n", paste(names(which(rs$which[which.min(subset_bic_values),]==TRUE)[-1]), collapse = ", "), "\n")
print(paste("The best model from AIC metric is:",which.min(subset_aic_values)))
print(paste("The best model from BIC metric is:",which.min(subset_bic_values)))
print(paste("The best model's AIC value is:",min(subset_aic_values)))
print(paste("The best model's BIC value is:",min(subset_bic_values)))

full_model_formula <- "life~ isSouthAmerica + isCentralAmerica + PC1 + PC2 + PC3 + PC4 + PC5 + PC6 "

full_model <- lm(full_model_formula, data = data_pca_6)
summary(full_model)
```
PCA can effectively reduce multicollinearity in the dataset by transforming correlated variables into a set of linearly uncorrelated components. However, the adjusted R-squared of the PCR model is 0.9616, lower than the previous model we selected. This might be caused by the fact that we've already mitigated the problem of multicollinearity by model selection. As is shown in the hot plot before, we can see that the variables we choose are hardly linearly related.
Also, PCA loses interpretation ability since its components are orthogonal vectors generated by the data.

## Variance Reduction with Ridge and Lasso
### Ridge
```{r, fig.height=4.5,fig.width=10, fig.pos='H', fig.align='center'}
library(glmnet)
X <- model.matrix(life ~ .-life-developed, data = data_clean_scaled)[, -1]
y <- data_clean_scaled$life
ols.mod <- lm(y ~ X)
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(y = y, x = X, alpha = 0, lambda = lambda.grid)
cv.ridge.mod <- cv.glmnet(x = X, y = y, alpha = 0, nfolds = 10)

par(mfrow=c(1, 2))
plot(ridge.mod, xvar = "lambda")
plot(cv.ridge.mod)

# We can find the lambda yielding the best fit, and the coefficients
best.ridge.lam <- cv.ridge.mod$lambda.min
best.ridge.coefs <- predict(ridge.mod, type = 'coefficients', s = best.ridge.lam)
print(paste("The best ridge lambda selected from the CV MSE is:",best.ridge.lam))
print(paste("The lowest CV MSE of the ridge model is:",min(cv.ridge.mod$cvm)))
```

\newpage

#### Lasso

```{r,fig.height=4.5,fig.width=10, fig.pos='H', fig.align='center'}
lasso.mod <- glmnet(x = X, y = y, alpha = 1)
cv.lasso.mod <- cv.glmnet(x = X, y = y, alpha = 1, nfolds = 10)

best.lasso.lam <- cv.lasso.mod$lambda.min
# Plot the lasso path on the lambda scale and add a line for the values at the best l ambda

par(mfrow=c(1, 2))
plot(cv.lasso.mod)
plot(lasso.mod, xvar = "lambda")
lines(c(log(best.lasso.lam), log(best.lasso.lam)),
c(-1000, 1000), lty = "dashed", lwd = 3)
best.lasso.coefs <- predict(lasso.mod, type = 'coefficients', s = best.lasso.lam)
```

```{r}
# see the variables selected by the best lambda
print(paste("The best lasso lambda selected from the CV MSE is:",best.lasso.lam))
print(paste("The lowest CV MSE of the lasso model is:",min(cv.lasso.mod$cvm)))
cat("Variables selected by Lasso includes:\n", paste(c(which(best.lasso.coefs[,1] != 0) %>% names())[-1], collapse = ", "))
```

Lasso and Ridge also use the stratefy of compromising bias for variance. Unlike AIC and BIC, which penalize on the number of variables, they directly penalize on the coefficients of the variables. The cross validation MSE achieved by lasso is much lower than ridge model. 
Also, Lasso model can be used as model selection strategy. Compared with AIC and BIC, Lasso includes more variables in the model selection. It is also worth noticing that the variables it chooses includes all the variables selected in previous models, proving that these variables are significant.

# Conclusion
## Objective
The project investigates the influence of factors like economic status, vaccination coverage, educational attainment, and health indicators on worldwide life expectancy using linear model's superiosity in inference and intepretability. Our goal doesn't focus on creating a linear model that precisely forecasts life expectancy based on several variables; it pinpoints and measures the effects of different social, economic, and health-related factors. The insights gained from this study could guide specific interventions and policy-making to improve health conditions around the world. To accomplish our objectives, employing diverse strategies learned in class, we developed two linear models. The first delivers precise predictions with a high adjusted R-squared value, while the second offers valuable insights from a different viewpoint.

## Model Construction Procedure
Our approach to building the model consists of four main phases: data preprocessing, model selection, regression diagnostics, and model comparison. During data preprocessing, we first examine the data and make up for the missing values. Considering the dependece between yearly data, our solution is averaging the data over years. Then, we employ box-cox transformations to reduce skewness. In the model selection phase, we use AIC and BIC metrics to guide our choices and apply F-tests for selecting interaction terms. Ultimately, we develop two distinct linear models by incorporating different dummy variables for regression focusing on differentiating the developing status of countries and the other on countries' regions. Additionally, we explored other techniques like PCA, Lasso, and Ridge regression to decrease the model's variance and address multicollinearity. We also the effectiveness of these models.

## Analysis and Insights from Model Coefficients
In our first model, we exclude the regional information and regress life expectency on Infant deaths, adult mortality, alcohol consumption, measles vaccine coverage, diphtheria vaccine coverage, population and development status and the interaction between development status and adult mortality and alcohol. 
Our second model include the regional factor and regress life expectency on infant deaths, adult mortality, population, measles vaccine coverage, diphtheria vaccine coverage, years in school and two dummy variables, indicating whether the country is in central America or south America.

Both models reveal that higher adult mortality and infant deaths negatively impact life expectancy, a finding that aligns with common understanding. A notable result is the positive correlation between diphtheria vaccine coverage and life expectancy, highlighting the significance of social healthcare services. Interestingly, a larger population seems to correlate with increased life expectancy, though the causality of this relationship is unclear; it might be that higher life expectancy leads to larger populations, or other confounding factors could be at play.

The first model unexpectedly suggests a positive correlation between alcohol consumption and life expectancy, a counterintuitive finding that merits further exploration. The second model shows the positive impact of education, indicated by the 'school' variable's positive coefficient.

The interaction terms in our first model merit further examination. Specifically, the positive interaction between adult mortality and development status implies that in developed countries, the detrimental effect of adult mortality on life expectancy might be mitigated. Conversely, the negative interaction involving alcohol consumption and development status suggests that the positive link between alcohol and life expectancy observed in the overall model may weaken or even invert in developed nations. This might account for the unexpected positive coefficient of 'alcohol': in developed regions, the overall impact of alcohol appears negative, potentially means alcohol is detrimental to one's health if abused. However, in developing countries, the access to alcohol means people's life condition is comparatively better, leading to higher life expectancy.

Contrary to expectations, both models indicate a negative correlation between measles vaccine coverage and life expectancy. Closer examination revealed that many measles data points were imputed with average regional figures or averaged over years due to missing original data, casting doubt on the reliability of the data and this variable's coefficient, necessitating further investigation.

## Model Comparison
In this project, we developed two distinct models, each with a unique objective. Both models demonstrate high efficacy in prediction, as evidenced by their adjusted R-squared values exceeding 0.98 while using fewer than 10 variables. This indicates the effectiveness of our models in forecasting with the existing dataset. Additionally, the valuable insights derived from our analysis have the potential to inform targeted strategies and policy decisions aimed at enhancing global health conditions.

# Discussion

In this project, we still have some limitations that can be further refined and imporved:

## Data Reliability

As highlighted in the **Conclusion** section, the coefficient for *measles* presents a conceptual challenge, potentially arising from our approach to handling missing values in the dataset. This issue extends beyond *measles*, as similar imputation methods were applied to other variables with missing data. Enhancing the accuracy and completeness of our dataset could substantially improve the model's predictive power and reliability.

## Time Series Analysis

Our dataset encompasses time series data, for which we employed an averaging method to mitigate dependency issues, based on observed linear trends over time. However, alternative approaches to handling time series data might yield more insightful results. We anticipate exploring these methods more thoroughly, particularly after systematically studying Time Series analysis, to enrich our understanding and model performance.

## Outlier Elimination

Regression analysis involved identifying and removing outliers to enhance the model's performance. We relied on standard residuals and Cook's distance, utilizing default parameters for outlier detection, such as a threshold of two times the standard residual. This generalized approach, without a detailed examination of each outlier, might introduce inaccuracies. Future work will involve a more nuanced analysis of outliers to refine our model's accuracy and robustness.

# Reference

- Lasha. Life Expectancy (WHO) - Updated. Kaggle. 2023. [Online]. Available: [https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated](https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated).

- Fox, John. Applied regression analysis and generalized linear models. Sage Publications, 2015.

- James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: Springer, 2013.


